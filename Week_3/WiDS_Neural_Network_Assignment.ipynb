{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejask-42/Speech-Emotion-Recognition-Project/blob/main/Week_3/WiDS_Neural_Network_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yIoqZ5ojmsA"
      },
      "source": [
        "Our objective is to build a neural network for the classification of the MNIST dataset. This neural network will comprise of: an output layer with 10 nodes, a hidden layer of 128 nodes, and an input layer with 784 nodes corresponding to the image pixels. The specific structure of the neural network is outlined below, where $X$ represents the input, $A^{[0]}$ denotes the first layer, $Z^{[1]}$ signifies the unactivated layer 1, $A^{[1]}$ stands for the activated layer 1, and so forth. The weights and biases are represented by $W$ and $b$ respectively:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "$A^{[0]}=X$\n",
        "\n",
        "$Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}$\n",
        "\n",
        "$A^{[1]}=\\text{ReLU}(Z^{[1]})$\n",
        "\n",
        "$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$\n",
        "\n",
        "$A^{[2]}=\\text{softmax}(Z^{[2]})$\n",
        "\n",
        "$Loss=\\text{cross-entropy-loss}(A^{[2]})$\n",
        "</div>\n",
        "\n",
        "You have the flexibility to create any function within or outside the class, allowing you to modify parameters as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zGlG63onjmsC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6rm0EBOjmsD"
      },
      "source": [
        "Now you will implement the activation function(ReLU) and softmax function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T_QQ9nMgjmsD"
      },
      "outputs": [],
      "source": [
        "def ReLU(Z):\n",
        "  return np.maximum(Z, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "  return np.exp(Z) / np.sum(np.exp(Z), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmbXsCx8jmsE"
      },
      "source": [
        "Now comes the important part.\n",
        "\n",
        "In this, you will implement the NN class, which will be the model you will be using to train data on and later use it to predict.\n",
        "\n",
        "You have been given the init function, you have to implement all the other functions yourself, in any way you like ... you may even skip some of them if you don't need them in the final implementation of the class.\n",
        "\n",
        "The description of each function has been given in the comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "R905_mXtjmsE"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # initialized basic stats of NN\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.output_size=output_size\n",
        "        self.learning_rate=learning_rate\n",
        "\n",
        "        #initialized weights and biases\n",
        "        self.W1=np.random.randn(hidden_size, input_size)*0.01\n",
        "        self.b1= np.zeros((hidden_size, 1))\n",
        "        self.W2=np.random.randn(output_size, hidden_size)*0.01\n",
        "        self.b2=np.zeros((output_size, 1))\n",
        "\n",
        "        #initialized activations and gradients\n",
        "        self.AO=None\n",
        "        self.Z1=None\n",
        "        self.A1=None\n",
        "        self.Z2=None\n",
        "        self.A2=None\n",
        "        self.dW2=None\n",
        "        self.db2=None\n",
        "        self.dW1=None\n",
        "        self.db1=None\n",
        "\n",
        "    # do the forward pass and evaluate the values of A0, Z1, A1, Z2, A2\n",
        "    def forward_propagation(self, X):\n",
        "        self.A0 = X\n",
        "        self.Z1 = np.dot(self.W1, self.A0) + self.b1\n",
        "        self.A1 = ReLU(self.Z1)\n",
        "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
        "        self.A2 = softmax(self.Z2)\n",
        "\n",
        "    # convert the input y, into a one hot encoded array.\n",
        "    '''\n",
        "    one hot encoding is:\n",
        "    you have an array with values [2, 5, 6] and you know the max value can be 8, then one hot encoded array will be:\n",
        "    [[0,0,1,0,0,0,0,0,0], [0,0,0,0,0,1,0,0,0], [0,0,0,0,0,0,1,0,0]]\n",
        "    Note that the index 2, 5, 6 have values 1 and all others have values 0\n",
        "    '''\n",
        "    def one_hot(self, y):\n",
        "      one_hot_y = np.zeros((self.output_size, y.size))\n",
        "      one_hot_y[y, np.arange(y.size)] = 1\n",
        "      return one_hot_y\n",
        "\n",
        "    # calculate the derivative of the loss function with respect to W2, b2, W1, b1 in dW2, db2, dW1, db1 respectively\n",
        "    def backward_propagation(self, X, y):\n",
        "        m = X.shape[1]\n",
        "        dz2 = self.A2 - y\n",
        "        self.dW2 = np.dot(dz2, self.A1.T) / m\n",
        "        self.db2 = np.sum(dz2, axis=1, keepdims=True) / m\n",
        "        dz1 = np.dot(self.W2.T, dz2) * (self.A1 > 0) # derivative of ReLU\n",
        "        self.dW1 = np.dot(dz1, X.T) / m\n",
        "        self.db1 = np.sum(dz1, axis=1, keepdims=True) / m\n",
        "\n",
        "    # update the parameters W1, W2, b1, b2\n",
        "    def update_params(self):\n",
        "        self.W1 -= self.learning_rate * self.dW1\n",
        "        self.b1 -= self.learning_rate * self.db1\n",
        "        self.W2 -= self.learning_rate * self.dW2\n",
        "        self.b2 -= self.learning_rate * self.db2\n",
        "\n",
        "    # get the predictions for the dataset\n",
        "    def get_predictions(self, X):\n",
        "        self.forward_propagation(X.T)\n",
        "        return np.argmax(self.A2, axis=0)\n",
        "\n",
        "    # get the accuracy of the model on the dataset\n",
        "    def get_accuracy(self, X, y):\n",
        "      self.forward_propagation(X.T)\n",
        "      a_out = np.argmax(self.A2, axis=0)\n",
        "      return np.sum(a_out == y) / y.size\n",
        "\n",
        "    # run gradient descent on the model to get the values of the parameters\n",
        "    def gradient_descent(self, X, y, iters=200, batch_size=64):\n",
        "      X = X.T\n",
        "      for i in range(iters):\n",
        "          indices = np.arange(X.shape[1])\n",
        "          np.random.shuffle(indices)\n",
        "          X = X[:, indices]\n",
        "          y = y[indices]\n",
        "\n",
        "          for start in range(0, X.shape[1], batch_size):\n",
        "              end = start + batch_size\n",
        "              X_batch = X[:, start:end]\n",
        "              y_batch = y[start:end]\n",
        "              y_one_hot = self.one_hot(y_batch)\n",
        "              self.forward_propagation(X_batch)\n",
        "              self.backward_propagation(X_batch, y_one_hot)\n",
        "              self.update_params()\n",
        "\n",
        "          if i % 10 == 0:\n",
        "              y_one_hot_full = self.one_hot(y)\n",
        "              cost = self.cross_entropy_loss(X, y_one_hot_full)\n",
        "              print(f\"Iteration number: {i}\")\n",
        "\n",
        "    # evaluate loss using cross-entropy-loss formula.\n",
        "    def cross_entropy_loss(self, X, y):\n",
        "        self.forward_propagation(X)\n",
        "        clipped_A2 = np.clip(self.A2, 1e-7, 1 - 1e-7)\n",
        "        loss = -np.sum(y * np.log(clipped_A2), axis=1) / y.shape[1]\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # Let me help a bit hehe :)\n",
        "    def show_predictions(self, X, y, num_samples=10):\n",
        "        random_indices = np.random.randint(0, X.shape[0], size=num_samples)\n",
        "\n",
        "        for index in random_indices:\n",
        "            sample_image = X[index, :].reshape((28, 28))\n",
        "            plt.imshow(sample_image, cmap='gray')\n",
        "            plt.title(f\"Actual: {y[index]}, Predicted: {self.get_predictions(X)[index]}\")\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-a9-AapjmsE"
      },
      "source": [
        "Now we are splitting the dataset into training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9io2U4RgjmsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94e119b-abc6-47bc-c2d1-896d45167ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "miu = np.mean(X_train, axis=(0, 1), keepdims=True)\n",
        "stds = np.std(X_train, axis=(0, 1), keepdims=True)\n",
        "\n",
        "mius = np.mean(X_test, axis=(0, 1), keepdims=True)\n",
        "stdse = np.std(X_test, axis=(0, 1), keepdims=True)\n",
        "\n",
        "X_normal_train = (X_train - miu) / (stds + 1e-7)\n",
        "X_normal_test = (X_test - mius) / (stdse + 1e-7)\n",
        "\n",
        "X_normal_train = X_normal_train.reshape((60000, -1))\n",
        "X_normal_test = X_normal_test.reshape((10000, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLbYf8VrjmsF"
      },
      "source": [
        "Now you will train the model on X_normal_train and Y_train dataset\n",
        "\n",
        "Then print the accuracy of model on X_normal_test and Y_test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aimj8-QgjmsF"
      },
      "outputs": [],
      "source": [
        "model = NN(input_size=784, hidden_size=128, output_size=10, learning_rate=0.01)\n",
        "model.gradient_descent(X_normal_train, Y_train)\n",
        "print(f\"Test accuracy: {model.get_accuracy(X_normal_test, Y_test)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}